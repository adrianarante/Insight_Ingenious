"""
perf/test_unicode_splitter_perf.py
==================================

Performanceâ€‘regression guard for *UnicodeSafeTokenTextSplitter*.

Background
----------
The original (preâ€‘optimisation) implementation inside
``UnicodeSafeTokenTextSplitter.split_text`` reâ€‘encoded the **entire** buffer
at every grapheme append, yielding *quadratic* behaviour â€“
``encode()`` was invoked â‰ˆâ€¯*len(text)Â²* times for long inputs.

The reâ€‘write keeps a *running token counter* and reâ€‘encodes only a **tiny
tail context** on each loop iteration, so the endâ€‘toâ€‘end complexity is now
*linear* in the number of grapheme clusters, i.e. **O(N)**.

Goal
----
Detect any future changes that accidentally drift back to *superâ€‘linear*
behaviour.  We monkeyâ€‘patch the underlying ``tiktoken.Encoding.encode``
method to count how many times it is called while performing a realistic
split.  The assertion below allows *at most* **4â€¯Ã—â€¯N** calls, where N is
the number of grapheme clusters in the input.

â€¢ Why 4â€¯Ã—â€¯N?
  â€“ Each iteration encodes two small strings (*before*Â +Â *after* context) â†’ 2â€¯Ã—â€¯N
  â€“ At every chunk flush we recompute overlap windows, costing **another**
    two calls.  The worstâ€‘case number of flushes is `< N / chunk_size`,
    so 4â€¯Ã—â€¯N is still a generous *linear* bound while remaining several
    ordersâ€‘ofâ€‘magnitude below the old quadratic profile.

If the algorithm regresses (e.g. someone reâ€‘introduces ``len(enc.encode())``
over the full buffer) the encodeâ€‘call count will exceed this threshold and
fail the test loudly in CI.

Usage
-----
Executed automatically by *pytest* under the ``tests/perf`` collection.
"""

from __future__ import annotations

import regex as re

from ingenious.chunk.strategy.langchain_token import UnicodeSafeTokenTextSplitter


def test_encode_call_count(monkeypatch) -> None:
    """
    Split a 12â€¯kâ€‘grapheme document and assert the underlying tokenizerâ€™s
    ``encode()`` method is called at most **4Â Ã—Â N** times (linear bound).

    The test is quick (~30â€¯ms on CI) and hermetic â€“ no network, no disk I/O.
    """
    # ---------------------------------------------------------------
    # 1. Build a mediumâ€‘size input: repeating "AðŸ˜€ " (3 graphemes) so
    #    we get a mix of ASCII and multiâ€‘codeâ€‘point emoji clusters.
    #    4Â 000 iterations â†’ 12Â 000 graphemes, enough to exhibit
    #    previously quadratic behaviour without slowing the test suite.
    # ---------------------------------------------------------------
    text: str = ("AðŸ˜€ " * 4000).strip()
    clusters = re.findall(r"\X", text)  # extended grapheme clusters
    n = len(clusters)

    # ---------------------------------------------------------------
    # 2. Instantiate the tokenizerâ€‘aware splitter under test.
    # ---------------------------------------------------------------
    splitter = UnicodeSafeTokenTextSplitter(
        encoding_name="cl100k_base",  # same default used in production
        chunk_size=64,  # tiny budget to force many flushes
        chunk_overlap=0,
        overlap_unit="tokens",
    )
    enc = splitter._enc  # tiktoken.Encoding instance (cached)

    # ---------------------------------------------------------------
    # 3. Monkeyâ€‘patch `enc.encode` with a counting wrapper so we can
    #    measure how many times the optimiser calls it.
    # ---------------------------------------------------------------
    call_counter = {"n": 0}
    original_encode = enc.encode

    def _counting_encode(s: str):  # noqa: D401 (simple helper)
        call_counter["n"] += 1
        return original_encode(s)

    monkeypatch.setattr(enc, "encode", _counting_encode)

    # ---------------------------------------------------------------
    # 4. Run the split â€“ *all* encode() invocations will now be tallied.
    # ---------------------------------------------------------------
    splitter.split_text(text)

    # ---------------------------------------------------------------
    # 5. Assertion â€“ linearâ€‘time guard.
    #    Allow up to 4Â Ã—Â N encode calls; anything above indicates an
    #    accidental O(NÂ²) regression.
    # ---------------------------------------------------------------
    max_allowed = 4 * n
    actual_calls = call_counter["n"]
    assert actual_calls <= max_allowed, (
        f"O(NÂ²) regression detected: encode() called {actual_calls:,} times "
        f"for {n:,} graphemes (limit {max_allowed:,})"
    )
